<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>涛声依旧</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description">
<meta property="og:type" content="website">
<meta property="og:title" content="涛声依旧">
<meta property="og:url" content="https://dingguitao.github.io/index.html">
<meta property="og:site_name" content="涛声依旧">
<meta property="og:description">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="涛声依旧">
<meta name="twitter:description">
  
    <link rel="alternative" href="/atom.xml" title="涛声依旧" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  <link rel="stylesheet" href="/css/style.css" type="text/css">
  

</head>
<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">涛声依旧</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" results="0" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="q" value="site:https://dingguitao.github.io"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main">
  
    <article id="post-OpenTSDB-Digest-time-series-data-table-design-on-hbase" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2015/06/27/OpenTSDB-Digest-time-series-data-table-design-on-hbase/" class="article-date">
  <time datetime="2015-06-27T07:35:59.000Z" itemprop="datePublished">2015-06-27</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Data-Technology/">Data Technology</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/06/27/OpenTSDB-Digest-time-series-data-table-design-on-hbase/">OpenTSDB Digest: HBase 中时间序列数据表的设计</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>最近一段时间在做实时数据监控相关的工作，过程中接触到了 <a href="http://opentsdb.net/" target="_blank" rel="external">OpenTSDB</a> 这个架在 Apache HBase 上的时间序列数据库，发现它针对时间序列数据的 HBase Schema 的设计比较巧妙，所以在这里梳理了相关的内容。</p>
<h2 id="关于时间序列数据（Time_Series_Data）">关于时间序列数据（Time Series Data）</h2><p>在介绍 OpenTSDB 之前，先简单介绍一下时间序列数据。按照 <a href="https://en.wikipedia.org/wiki/Time_series" target="_blank" rel="external">Wikipedia</a> 上的定义，时间序列数据一般指的是一系列的数据点，每个数据点包括数据值和时间值，像最近很火的股市中的K线图、平时用到的操作系统的CPU负载等，都与时间序列数据紧密联系。时间序列数据在实时监控、走势预测上有着很重要的作用，而时间序列数据库做的工作就是能够很高效地完成这些数据的存储和提取。</p>
<h2 id="关于OpenTSDB">关于OpenTSDB</h2><p>下图为OpenTSDB官网上的架构图，OpenTSDB通过一个或多个TSD（Time Series Daemon）接受数据的读写请求，然后TSD对HBase中的数据进行相应的读写操作，OpenTSDB本身提供了SOCKET和HTTP的API。</p>
<p><img src="http://7xjkvk.com1.z0.glb.clouddn.com/tsdb-architecture.png" alt="OpenTSDB Architecture"></p>
<p>OpenTSDB中的一个时间序列数据点包含以下的元素：</p>
<ul>
<li>metric：该数据点的指标名称，string 类型</li>
<li>timestamp：该数据点的Unix时间戳，秒级或者毫秒级，long 类型</li>
<li>value：该数据点的数值，int 或者 float 类型</li>
<li>tags：该数据点的属性，为一组或者多组键值对，键（tagk）和值（tagv）都是 string 类型</li>
</ul>
<p>例如在北京时间2015-06-27 17:08:15这个时间点网站上海分站的首页请求数为12345，该数据点对应的各元素可以是：</p>
<ul>
<li>metric：website.request</li>
<li>timestamp：1435396095</li>
<li>value：12345</li>
<li>tags：city=shanghai,page=homepage</li>
</ul>
<p>下面简单介绍一下OpenTSDB中几个不错的设计。</p>
<h2 id="Tags">Tags</h2><p>在 OpenTSDB 中，数据写入时每个数据点必须包含至少一个 tag，在数据查询的时候，用户可以不对 tag 进行过滤，也可以对一个或多个 tag 进行过滤，如果对一个结果时间点返回多个数据的话，用户可以选择对这些数据进行特定的聚合，OpenTSDB 提供了包括 sum、avg、min 和 max 在内的多种<a href="http://opentsdb.net/docs/build/html/user_guide/query/aggregators.html" target="_blank" rel="external">聚合方法</a>。</p>
<p>还是以上面的请求数的数据为例，这个数据点有 city 和 page 两个 tags，在查询的时候，可以同时满足以下几种场景的需求：</p>
<ul>
<li>查询<strong>上海分站首页</strong>的请求数：过滤条件对 tags 限制 city = shanghai 且 page = homepage</li>
<li>查询<strong>上海分站所有页面</strong>的请求数：过滤条件对 tags 限制 city = shanghai</li>
<li>查询<strong>网站所有分站的首页</strong>的请求数：过滤条件对 tags 限制 page = homepage</li>
<li>查询<strong>网站总</strong>的请求数：过滤条件中不对 tags 作限制</li>
</ul>
<p>在查询的时候，OpenTSDB 会把 tags 符合既定过滤条件的所有数据点输出并聚合，而忽略掉过滤条件中不包含的tags，这样我们只需要存一份数据，就可以完成多种不同维度上的数据分析，并且 OpenTSDB HBase Schema 的设计能够保证这种类型查询的高效率完成（下面会解释）。</p>
<p><strong><em>NOTE:</em></strong> <em>对于那些需要去重的指标，tags是不适用的，因为 OpenTSDB 现有的聚合方法中只是对数字的聚合，并不会考虑去重的情况。如果需要统计这样的指标，每一种维度都应该要对应一个独立的metric，例如：website.request.uv.all、website.request.uv.by.city、website.request.uv.by.page等。</em></p>
<h2 id="UID（Unique_Identifier）">UID（Unique Identifier）</h2><p>OpenTSDB 为每一种 metric、tagk 和 tagv 的值都分别设定了一个独立的 UID，UID 的 mapping 信息存在 tsdb-uid 表中，UID 的分配是自动的（metric 的自动分配需要手动设定 auto metric 为true），例如当 OpenTSDB 第一次遇到 city=shanghai, page=homepage 这个 tags 的时候，它会分别分配值为 1 和 2 的 tagk UID 给 “city” 和 “page”，然后分别分配值为 1 和 2 的 tagv UID 给 “shanghai” 和 “homepage”。</p>
<p>默认情况下，每个 UID 的存储占用 3 个字节，即对每种类型的 UID 来讲，默认允许的最大独立 UID 数为 2^(8*3) - 1 = 16,777,215 个，如果这个数不够用的话（比如 tagv 的 cardinality 非常高），可以修改源代码。因为使用 UID 代替原始的字符串值进行存储（这跟关系型数据库中的 star schema 有些类似），所以在数据写入的时候要多做一次 UID mapping 的工作，效率可能稍有下降，但是数据存储占用的磁盘空间降低了很多，数据读取的带宽和 IO 消耗也相应地有所减少。</p>
<h2 id="TSUID（Time_Series_Unique_Indentifier）">TSUID（Time Series Unique Indentifier）</h2><p>在 OpenTSDB 中，每个数据点都会有对应一个 TSUID（事实上，数据点与 TSUID 之间是多对一的关系），时间序列的所有数据存放在 tsdb 一张表中，TSUID 即为表的 ROW KEY，TSUID 是由 metric、timestamp、tags 构成的：</p>
<p><code>&lt;metric_UID&gt;&lt;timestamp&gt;&lt;tagk1_UID&gt;&lt;tagv1_UID&gt;[...&lt;tagkN_UID&gt;&lt;tagvN_UID&gt;]</code></p>
<p>还是以我们之前的数据点为例子，这个数据点对应的TSUID如下图所示：</p>
<p><img src="http://7xjkvk.com1.z0.glb.clouddn.com/tsuid.png" alt="TSUID Example"></p>
<p>通过这样的 ROW KEY 设计，OpenTSDB在查询数据的时候，可以很高效地从 ROW KEY 中获得 metric、timestamp、tags的信息。事实上，OpenTSDB 生成 TSUID的过程中，timestamp 是小时级别的，即对同样 tags 的同一个 metric 的数据，一小时内的所有数据点（因为可以精确到毫秒，所以最多可能有60 <em> 60 </em> 1000 = 3.6M 个）只对应 tsdb 表中的一行，这样一定程度上发挥了 HBase NoSQL 宽表的优势，提高了数据查询的效率。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://dingguitao.github.io/2015/06/27/OpenTSDB-Digest-time-series-data-table-design-on-hbase/" data-id="cibj9md47000ycrowg41ff3ww" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/HBase/">HBase</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/OpenTSDB/">OpenTSDB</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Time-Series-Database/">Time Series Database</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-Feature-Engineering-using-Apache-Hive-HBase-and-Phoenix" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2015/06/07/Feature-Engineering-using-Apache-Hive-HBase-and-Phoenix/" class="article-date">
  <time datetime="2015-06-07T04:23:17.000Z" itemprop="datePublished">2015-06-07</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Data-Technology/">Data Technology</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/06/07/Feature-Engineering-using-Apache-Hive-HBase-and-Phoenix/">从ODS到FEATURE：Apache Hive、HBase、Phoenix三剑客之离线特征工程实战</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>一个互联网公司，随着业务的不断深入和细化，一般会在数据上对特征类数据的需求会越来越大：机器学习需要大量的对象特征数据来训练模型、半精准或精准化营销需要根据对象特征来进行用户分群、推荐系统需要知道对象特征数据才能完成匹配等等，这么强烈的需求给数据工程师提出了一个比较大的挑战。</p>
<p>我现在的门特尔泉哥常说：现在是一个最好的时代，因为有那么多轮子可用。的确，“大数据”的火热让数据社区越来越热闹，这也使得一些规模不太大的数据团队可以借助开源的力量完成一些比较挑战的工作。本文（确切说是本人）不会讲任何关于“大数据”的任何内容，只是结合最近做的项目，记录一下使用Apache Hive、HBase和Phoenix这三剑客在特征工程上的实战经历。</p>
<h2 id="关于“特征工程”">关于“特征工程”</h2><p>特征工程（Feature Engineering）是一个很宽泛的话题，一般大家都认为特征工程是机器学习的前奏，它的主要工作是根据场景计算和选择输入的x变量。当大多数的精力都放在炫酷的数学公式和模型的时候，人们往往会忽视特征工程的重要性，其实特征工程也包含很多内容：特征定义、特征加工、特征提取和特征选择等等，本文的内容会集中在离线环境下的特征加工和特征提取这两个方面。</p>
<p><strong>特征加工（Feature Construction）：</strong>特征必须是依附在一个对象（Entity）身上的，一个对象身上会有多个特征，对象之间和特征之间都是相互独立的。这里说的特征加工的范围是根据当前对象的可操作数据（ods）计算得到该对象的特征（feature），用数据人的语言（SQL）表示是这样的：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator"><span class="keyword">select</span></span><br><span class="line">    <span class="keyword">coalesce</span>(f0001.entity_id, f0002.entity_id, ... , f1234.entiry_id) <span class="keyword">as</span> entity_id,</span><br><span class="line">    f0001.fea_0001,</span><br><span class="line">    f0002.fea_0002,</span><br><span class="line">    ... ,</span><br><span class="line">    f1234.fea_1234</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">    (<span class="keyword">select</span></span><br><span class="line">        entity_id,</span><br><span class="line">        agg(<span class="keyword">value</span>) <span class="keyword">as</span> feature_0001</span><br><span class="line">    <span class="keyword">from</span></span><br><span class="line">        ods.table_0001</span><br><span class="line">    <span class="keyword">group</span> <span class="keyword">by</span></span><br><span class="line">        entiry_id</span><br><span class="line">    ) f0001</span><br><span class="line">    <span class="keyword">full</span> <span class="keyword">outer</span> <span class="keyword">join</span></span><br><span class="line">    (</span><br><span class="line">        ...</span><br><span class="line">    ) f0002</span><br><span class="line">    <span class="keyword">on</span> f0001.entity_id = f0002.entity_id</span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">full</span> <span class="keyword">outer</span> <span class="keyword">join</span></span><br><span class="line">    (</span><br><span class="line">        ...</span><br><span class="line">    ) f1234</span><br><span class="line">    ...</span></span><br></pre></td></tr></table></figure>
<p><strong>特征提取（Feature Extraction）：</strong>根据不同的场景，离线环境下特征数据的提取过程大同小异，基本的模式是输出某些特征符合给定条件的所有对象的ID（entity_id）和兴趣特征值，用数据人的语言表示是这样的：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator"><span class="keyword">select</span></span><br><span class="line">    entity_id,</span><br><span class="line">    fea_0002,</span><br><span class="line">    fea_0520,</span><br><span class="line">    fea_1111</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">    feature.<span class="keyword">table</span></span><br><span class="line"><span class="keyword">where</span></span><br><span class="line">    fea_0013 &gt; <span class="number">1.2</span> <span class="keyword">and</span></span><br><span class="line">    fea_0014 &lt; <span class="number">2.3</span> <span class="keyword">and</span></span><br><span class="line">    fea_0064 <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">null</span></span></span><br></pre></td></tr></table></figure>
<p>值得注意的是，一种对象的特征可能有成千上万个，所以数据是相当稀疏（sparse）的，而且很多场景下筛选用到（where）的或者感兴趣提取（select）的特征可能只有很少的几个，这就需要保证整个提取引擎的水平扩展性要非常好，最后我们的系统设计基本如下图所示，有耐心的童鞋看完图可以接着往下看：）</p>
<p><img src="http://7xjkvk.com1.z0.glb.clouddn.com/feature_engineering_architecture.png" alt="Feature Engineering Architecture"></p>
<h2 id="使用Hive进行单个特征的加工">使用Hive进行单个特征的加工</h2><p>单个特征的加工本质上是一个基于entity_id的聚合过程，在我们的项目中，它的输入数据是原始或者经过清洗的日志数据，输出则是每个entity_id对应的特征值。如果整个数据仓库的环境是基于HDFS的，那么MapReduce非常适合做这个工作，别忘了，著名的MapReduce入门程序WordCount就是这样一个聚合的过程。</p>
<p><a href="http://hive.apache.org/" target="_blank" rel="external">Apache Hive</a>作为一个SQL on hadoop的工具，已经成为越来越多人做数据仓库的选择。Hive可以把Hive SQL翻译成MapReduce程序来查询HDFS上的数据，这让我们的开发过程会更加高效。比如说我们现在要从原始的流量数据（hive表名：ods.traffic）中加工出一个<em>最近七日用户PV总和（tag名：pv_last_7d_sum）</em>的特征，可以这么做：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 预聚合数据（这张表可以使用dt作为partition，然后每日增量更新即可）</span></span><br><span class="line"><span class="operator"><span class="keyword">create</span> <span class="keyword">table</span> user_pv_by_day <span class="keyword">as</span></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">    dt,</span><br><span class="line">    user_id,</span><br><span class="line">    <span class="keyword">count</span>(<span class="number">1</span>) pv</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">    ods.traffic</span><br><span class="line"><span class="keyword">where</span></span><br><span class="line">    dt <span class="keyword">between</span> <span class="string">'start_dt'</span> <span class="keyword">and</span> <span class="string">'end_dt'</span></span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span></span><br><span class="line">    dt,</span><br><span class="line">    user_id;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">-- 特征数据</span></span><br><span class="line"><span class="operator"><span class="keyword">create</span> <span class="keyword">table</span> user_pv_last_7d_sum <span class="keyword">as</span></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">    user_id,</span><br><span class="line">    <span class="keyword">sum</span>(pv) <span class="keyword">as</span> user_pv_last_7d_sum</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">    user_pv_by_day</span><br><span class="line"><span class="keyword">where</span></span><br><span class="line">    dt <span class="keyword">between</span> <span class="string">'start_dt'</span> <span class="keyword">and</span> <span class="string">'end_dt'</span></span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span></span><br><span class="line">    user_id;</span></span><br></pre></td></tr></table></figure>
<h2 id="使用HBase将所有特征关联">使用HBase将所有特征关联</h2><p>单个的特征加工完成之后，我们得到了很多的特征小表（每个特征对应一张），下面就需要根据特征所属的entity_id把同一个对象的所有特征关联起来，得到一张特征的“大表”。如果继续使用Hive，那么这就是一个full join的工作，join的表的数量跟特征的数量相同，因为独立特征数可能会达到几千甚至几万，显然MapReduce或者现阶段的Hive已经不适合做这个工作了。</p>
<p>回到我们得到的特征小表的结构：</p>
<ul>
<li>每张小表只有两列：entity_id和一个特征。</li>
<li>每张小表中entity_id都是唯一的，可以当做Primary Key。</li>
<li>数据是稀疏的，大多数的entity_id都是只有部分特征有值。</li>
</ul>
<p>再思考一下特征大表的使用场景：</p>
<ul>
<li>特征可能随时增加或删减，这要求大表的结构要有足够的灵活性。</li>
<li>查询的复杂度最好只跟查询时用到的特征的数量有关，而与存储的特征的总数无关。</li>
<li>因为要提供对外的数据服务，查询的延迟（latency）要尽量低。</li>
</ul>
<p>熟悉<a href="https://hbase.apache.org/" target="_blank" rel="external">Apache HBase</a>的同学发现，HBase天然适合做这件事，建立一张有一个或多个列族的特征大表，entity_id作为row_key，每个特征作为一个列族的一个列，这样特征的关联就是简单的UPSERT操作，HBase本身的查询延迟就非常低，它的<a href="http://0b4af6cdc2f0c5998459-c0245c5c937c5dedcca3f1764ecc9b2f.r43.cf2.rackcdn.com/9353-login1210_khurana.pdf" target="_blank" rel="external">数据模型</a>也保证了查询的复杂度跟表的列数没有关系：</p>
<blockquote>
<p>You can view the same thing as if it’s a key-value store, where the key is the row key and the value is the rest of the data in a column . Given that the row key is the only way to address a row, that seems befitting . You can also consider HBase to be a key-value store where the key is defined as row key, column family, column qualifier, timestamp, and the value is the actual data stored in the cell .</p>
</blockquote>
<p>因为之前的特征小表是在Hive中生成的，所以需要通过<a href="https://cwiki.apache.org/confluence/display/Hive/HBaseIntegration" target="_blank" rel="external">hive-hbase-handler</a>将Hive中的数据写入HBase。比如说我们现在有了两个特征：<em>最近七日用户PV总和（特征名：pv_last_7d_sum）</em>和<em>用户注册日期（特征名：reg_dt）</em>，可以这么做：</p>
<p>首先需要在HBase建好大表：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator"><span class="keyword">create</span> <span class="string">'FEATURE'</span>, &#123;NAME =&gt; <span class="string">'0'</span>, VERSIONS =&gt; <span class="number">1</span>&#125;</span></span><br></pre></td></tr></table></figure>
<p>然后将Hive中的数据写入HBase大表中：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 特征1：pv_last_7d_sum</span></span><br><span class="line"><span class="operator"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> user_pv_last_7d_sum_temp;</span></span><br><span class="line"><span class="operator"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> user_pv_last_7d_sum_temp (</span><br><span class="line">    user_id <span class="keyword">STRING</span>,</span><br><span class="line">    pv_last_7d_sum <span class="keyword">STRING</span></span><br><span class="line">)</span><br><span class="line">stored <span class="keyword">by</span> <span class="string">'org.apache.hadoop.hive.hbase.HBaseStorageHandler'</span></span><br><span class="line"><span class="keyword">with</span> serdeproperties (<span class="string">'hbase.columns.mapping'</span> = <span class="string">':key, 0:PV_LAST_7D_SUM'</span>)</span><br><span class="line">tblproperties (<span class="string">'hbase.table.name'</span> = <span class="string">'FEATURE'</span>);</span></span><br><span class="line"></span><br><span class="line"><span class="operator"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> user_pv_last_7d_sum_temp</span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">    user_id,</span><br><span class="line">    pv_last_7d_sum</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">    user_pv_last_7d_sum;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">-- 特征2： reg_dt</span></span><br><span class="line"><span class="operator"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> user_reg_dt_temp;</span></span><br><span class="line"><span class="operator"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> user_reg_dt_temp (</span><br><span class="line">    user_id <span class="keyword">STRING</span>,</span><br><span class="line">    reg_dt <span class="keyword">STRING</span></span><br><span class="line">)</span><br><span class="line">stored <span class="keyword">by</span> <span class="string">'org.apache.hadoop.hive.hbase.HBaseStorageHandler'</span></span><br><span class="line"><span class="keyword">with</span> serdeproperties (<span class="string">'hbase.columns.mapping'</span> = <span class="string">':key, 0:REG_DT'</span>)</span><br><span class="line">tblproperties (<span class="string">'hbase.table.name'</span> = <span class="string">'FEATURE'</span>);</span></span><br><span class="line"></span><br><span class="line"><span class="operator"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> user_reg_dt_temp</span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">    user_id,</span><br><span class="line">    reg_dt</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">    user_reg_dt;</span></span><br></pre></td></tr></table></figure>
<p>使用hive-hbase-handler的时候需要注意两点：</p>
<ul>
<li>列的数据类型全部都是string类型，其它类型的数据会在写入HBase之前转化为string类型。</li>
<li>每个版本的Hive自带的hive-hbase-handler都针对一个特定版本的HBase，例如1.2.0版本Hive对应的HBase版本是0.98.5，如果实际生产环境中用到了不同版本的HBase，则需要手动编译打包一下hive-hbase-handler.jar文件。</li>
</ul>
<h2 id="使用Phoenix提供特征提取的接口">使用Phoenix提供特征提取的接口</h2><p>到这里，我们已经完成了特征加工的工作，接下来是特征的提取，如前面提到的，现有场景下的特征提取过程实际是一个全表扫描的过程，而HBase是为随机读的场景设计的，一个SCAN的执行实际上是对region file的顺序扫描，所以我们如果想要提高全表扫描的效率，首先得实现HBase的并行查询，而<a href="http://phoenix.apache.org/" target="_blank" rel="external">Apache Phoenix</a>很好地解决了这个问题。</p>
<p>我之前的一篇文章已经对Phoenix做了一些简单的介绍，简单来讲Phoenix允许我们使用SQL查询HBase中的数据，使得我们的开发量大大降低，并且提供了很多额外的特性让HBase的全表扫描变得更加高效：</p>
<ul>
<li>通过coprocessors实现并行扫描。</li>
<li>对row_key加盐，避免hotspotting。</li>
<li>可以对非row_key列建立二级索引。</li>
</ul>
<p>另外，Phoenix还允许我们添加自定义函数，使得数据的存取更加灵活。4.4版本之后，Phoenix发行了自己的Query Server，用户可以直接通过JDBC的方式连接到Query Server上查询数据，为数据提取接口的开发提供了很多的方便。Phoenix与HBase之间的连接也非常简单，以我们之前建的特征大表为例，我们可以按照下面的步骤在Phoenix中建表，就可以直接写SQL查询特征数据了。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 1. 创建Phoenix View，完成与HBase特征大表的mapping</span></span><br><span class="line"><span class="operator"><span class="keyword">drop</span> <span class="keyword">view</span> <span class="keyword">if</span> <span class="keyword">exists</span> feature;</span></span><br><span class="line"><span class="operator"><span class="keyword">create</span> <span class="keyword">view</span> feature (</span><br><span class="line">    user_id <span class="built_in">varchar</span> <span class="keyword">primary</span> <span class="keyword">key</span>,</span><br><span class="line">    <span class="string">"0"</span>.pv_last_7d_sum <span class="built_in">varchar</span>,</span><br><span class="line">    <span class="string">"0"</span>.reg_dt <span class="built_in">varchar</span></span><br><span class="line">);</span></span><br><span class="line"></span><br><span class="line"><span class="comment">-- 2. 将特征数据复制到新的Phoenix Table</span></span><br><span class="line"><span class="operator"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> feature_final;</span></span><br><span class="line"><span class="operator"><span class="keyword">create</span> <span class="keyword">table</span> feature_final (</span><br><span class="line">    user_id <span class="built_in">varchar</span> <span class="keyword">primary</span> <span class="keyword">key</span>,</span><br><span class="line">    pv_last_7d_sum <span class="built_in">decimal</span>,</span><br><span class="line">    reg_dt <span class="built_in">varchar</span></span><br><span class="line">) SALT_BUCKETS=<span class="number">10</span>;</span></span><br><span class="line"></span><br><span class="line">upsert into</span><br><span class="line">    feature_final</span><br><span class="line"><span class="operator"><span class="keyword">select</span></span><br><span class="line">    user_id,</span><br><span class="line">    to_number(pv_last_7d_sum),</span><br><span class="line">    reg_dt</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">    feature_final;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">-- 3.查询最近七天PV总和大于100的用户的ID和注册日期</span></span><br><span class="line"><span class="operator"><span class="keyword">select</span></span><br><span class="line">    user_id,</span><br><span class="line">    reg_dt</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">    feature_final</span><br><span class="line"><span class="keyword">where</span></span><br><span class="line">    pv_last_7d &gt; <span class="number">100</span>;</span></span><br></pre></td></tr></table></figure>
<p>再建一张Phoenix的特征大表主要是因为Phoenix的View还不够稳定，另外这样也有几个好处：</p>
<ul>
<li>可以实现读写分离，避免行级事务带来的一些问题。</li>
<li>可以更方便地想用Phoenix的二级索引、row_key加盐等特性。</li>
<li>数据查询的效率比用View要高，测试下来，对~4KW行 x ~100列的数据表（总数据量在100G左右），在使用自定义函数完成很复杂运算的条件下，也能在1min左右给出查询结果。</li>
</ul>
<p>在这样的系统设计下，增加一个特征需要做的只是写几句HiveQL和调整Phoenix特征大表的表结构，可扩展性还是比较OK的。</p>
<p>以上就是本人使用Apache三剑客进行特征工程的实战经历，值得高兴的是这三剑客还在不断更新，相信它们会更强大更稳定。其实在整个过程中我也遇到了不少的坑，尤其是Phoenix和HBase的配置和优化，还好社区里的小伙伴们都很热心，所以整体的体验还是很愉快的，也希望写的这些能对大家的工作有所帮助。Happy Dataing!</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://dingguitao.github.io/2015/06/07/Feature-Engineering-using-Apache-Hive-HBase-and-Phoenix/" data-id="cibj9md370000crowniui9ooe" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Feature-Construction/">Feature Construction</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Feature-Engineering/">Feature Engineering</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Feature-Extraction/">Feature Extraction</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/HBase/">HBase</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Hive/">Hive</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Phoenix/">Phoenix</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-change-phoenix-server-request-encoding-to-utf8" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2015/05/28/change-phoenix-server-request-encoding-to-utf8/" class="article-date">
  <time datetime="2015-05-28T11:27:36.000Z" itemprop="datePublished">2015-05-28</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Data-Technology/">Data Technology</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/05/28/change-phoenix-server-request-encoding-to-utf8/">Apache Phoenix Server中文乱码问题解决</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>继上次解决Phoenix Server Header长度受限的问题之后，今天又发现了4.4版本的Phoenix Server对含有中文的查询并不友好（新出的东西还是慎用啊），虽然问题的解决方法比较简单，但是为了保持自己写文章的动力（第一次发现自己能把刷文章数辣么猥琐的事情说的这么清新脱俗），还是在这记录一下。</p>
<h2 id="问题表现">问题表现</h2><p>今天遇到几个在Phoenix Server的查询返回的结果数都为0，虽然没报错，但是凭感觉好像有点不对劲，就把同样的查询扔到Phoenix的终端（<code>sqline.py zookeeper</code>）重新跑了一下，果然是有数据返回的。再看之前Server上跑过的几个空结果的查询，发现一个共同的特点就是他们的SQL的Where语句中都包含中文字符，就像下面这样的：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator"><span class="keyword">select</span></span><br><span class="line">    col1,</span><br><span class="line">    col2</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">    <span class="keyword">table</span></span><br><span class="line"><span class="keyword">where</span></span><br><span class="line">    col0 = <span class="string">'中文'</span></span></span><br></pre></td></tr></table></figure>
<p>目测应该中文编码出了问题，为了确认，拿了下面这段SQL验证：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator"><span class="keyword">select</span></span><br><span class="line">    <span class="string">'中文'</span> <span class="keyword">as</span> abc_中文</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">    <span class="keyword">table</span></span><br><span class="line"><span class="keyword">limit</span></span><br><span class="line">    <span class="number">1</span></span></span><br></pre></td></tr></table></figure>
<p>果然返回结果里带”中文“的地方都变成了乱码。</p>
<h2 id="问题定位">问题定位</h2><p>第一反应去翻Phoenix Server的log，发现默认的情况下并没有记录接受到的请求内容，所以故技重施，直接修改<code>phoenix-server-4.4.0-HBase-1.0-runnable.jar</code>这个jar包中的类文件。因为Phoenix Server是基于Avatica的，Avatica是通过<code>org.apache.calcite.avatica.server.AvaticaHandler</code>这个类来处理请求的，日志部分的修改如下：</p>
<p>修改前：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">final</span> String jsonRequest = request.getHeader(<span class="string">"request"</span>);</span><br><span class="line"><span class="keyword">if</span> (LOG.isTraceEnabled()) &#123;</span><br><span class="line">    LOG.trace(<span class="string">"request: "</span> + jsonRequest);</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">final</span> String jsonResponse = jsonHandler.apply(jsonRequest);</span><br><span class="line"><span class="keyword">if</span> (LOG.isTraceEnabled()) &#123;</span><br><span class="line">    LOG.trace(<span class="string">"response: "</span> + jsonResponse);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>修改后：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">final</span> String jsonRequest = request.getHeader(<span class="string">"request"</span>);</span><br><span class="line">LOG.info(<span class="string">"request: "</span> + jsonRequest);</span><br><span class="line"><span class="keyword">final</span> String jsonResponse = jsonHandler.apply(jsonRequest);</span><br><span class="line">LOG.info(<span class="string">"response: "</span> + jsonResponse);</span><br></pre></td></tr></table></figure>
<p>重新打包，重启Phoenix Server，然后再次运行上面的测试SQL语句，翻log发现被解码成乱码的中文字符，也就是说上面java代码中第一句中的jsonRuest字符串已经编码不正确了。</p>
<h2 id="问题解决">问题解决</h2><p>问题解决比较简单，直接把<code>org.apache.calcite.avatica.server.AvaticaHandler</code>类中request header的编码修改成UTF-8编码就好了，具体如下：</p>
<p>修改前：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">final</span> String jsonRequest = request.getHeader(<span class="string">"request"</span>);</span><br></pre></td></tr></table></figure>
<p>修改后：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">final</span> String jsonRequest =  <span class="keyword">new</span> String(request.getHeader(<span class="string">"request"</span>).getBytes(<span class="string">"ISO-8859-1"</span>), <span class="string">"UTF-8"</span>);</span><br></pre></td></tr></table></figure>
<p>重新打包，重启Phoenix Server，搞定。</p>
<p>更新（2015年06月16日）：关于这个问题，我提给Apache Calcite的<a href="https://github.com/apache/incubator-calcite/pull/89" target="_blank" rel="external">PR</a>已经被合了：）</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://dingguitao.github.io/2015/05/28/change-phoenix-server-request-encoding-to-utf8/" data-id="cibj9md40000mcrow712q7cwi" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Avatica/">Avatica</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Encoding/">Encoding</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Phoenix/">Phoenix</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-fix_phoenix_server_error_413" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2015/05/26/fix_phoenix_server_error_413/" class="article-date">
  <time datetime="2015-05-26T08:04:07.000Z" itemprop="datePublished">2015-05-26</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Data-Technology/">Data Technology</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/05/26/fix_phoenix_server_error_413/">Apache Phoenix Server 提示 Header 过长（code 413）的解决办法</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>Phoenix 4.4版本发布之后，Phoenix的<a href="http://phoenix.apache.org/server.html" target="_blank" rel="external">Query Server</a>也终于浮出水面。在尝试使用Phoenix Server的时候遇到了一个服务器响应代码为413的错误，经过大半天的折腾终于解决了，特此记录一下。</p>
<h2 id="错误表现">错误表现</h2><p>在通过JDBC的方式连接到Phoenix Query Server之后，有的查询可以正常执行，有的会直接抛错：</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">response <span class="tag">code</span> <span class="number">413</span></span><br></pre></td></tr></table></figure>
<p>在Http的<a href="http://www.w3.org/Protocols/rfc2616/rfc2616-sec10.html" target="_blank" rel="external">状态码</a>中，413代表的是请求提交的数据量太大，服务器关闭了连接：</p>
<figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">413</span> Request Entity Too Large</span><br><span class="line">The server is refusing <span class="built_in">to</span> <span class="built_in">process</span> <span class="operator">a</span> request because <span class="operator">the</span> request entity is larger than <span class="operator">the</span> server is willing <span class="operator">or</span> able <span class="built_in">to</span> <span class="built_in">process</span>. The server MAY <span class="built_in">close</span> <span class="operator">the</span> connection <span class="built_in">to</span> prevent <span class="operator">the</span> client <span class="built_in">from</span> continuing <span class="operator">the</span> request.</span><br><span class="line">If <span class="operator">the</span> condition is temporary, <span class="operator">the</span> server SHOULD <span class="built_in">include</span> <span class="operator">a</span> Retry- After header field <span class="built_in">to</span> indicate that <span class="keyword">it</span> is temporary <span class="operator">and</span> <span class="keyword">after</span> what <span class="built_in">time</span> <span class="operator">the</span> client MAY <span class="keyword">try</span> again.</span><br></pre></td></tr></table></figure>
<h2 id="错误根源">错误根源</h2><p>知道了错误的原因，下面去找错误的“发源地”，查看Phoenix Server的log后发现了这个：</p>
<figure class="highlight groovy"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">2015</span>-<span class="number">05</span>-<span class="number">26</span> <span class="number">10</span>:<span class="number">07</span>:<span class="number">45</span>,<span class="number">703</span> WARN org.eclipse.jetty.http.<span class="string">HttpParser:</span> Header is too large &gt;<span class="number">8192</span></span><br><span class="line"><span class="number">2015</span>-<span class="number">05</span>-<span class="number">26</span> <span class="number">10</span>:<span class="number">07</span>:<span class="number">45</span>,<span class="number">704</span> WARN org.eclipse.jetty.http.<span class="string">HttpParser:</span> <span class="string">badMessage:</span> <span class="number">413</span> <span class="keyword">for</span> HttpChannelOverHttp@<span class="number">25</span>f0493&#123;r=<span class="number">2</span>,c=<span class="literal">false</span>,a=IDLE,uri=-&#125;</span><br><span class="line"><span class="number">2015</span>-<span class="number">05</span>-<span class="number">26</span> <span class="number">10</span>:<span class="number">07</span>:<span class="number">45</span>,<span class="number">704</span> WARN org.eclipse.jetty.http.<span class="string">HttpParser:</span> <span class="string">badMessage:</span> java.lang.<span class="string">IllegalStateException:</span> too much data after closed <span class="keyword">for</span> HttpChannelOverHttp@<span class="number">25</span>f0493&#123;r=<span class="number">2</span>,c=<span class="literal">true</span>,a=COMPLETED,uri=/&#125;</span><br></pre></td></tr></table></figure>
<p>这个错是Jetty抛出来的，原来Phoenix的Query Server是基于<a href="https://github.com/apache/incubator-calcite/blob/master/doc/avatica.md" target="_blank" rel="external">Avatica</a>的，而Avatica又是基于Jetty Http Server的（你们三个关系这么复杂，可以去拍肥皂剧了好嘛），Jetty的Http Server限制了Header的最大长度为8KB，如果打给Phoenix Server的SQL过长，请求的长度超过8KB，就会导致Jetty直接抛异常出来。</p>
<h2 id="错误解决">错误解决</h2><p>知道了错误的根源，下面就开始着手解决这个问题，首先想到的是有没有地方可以直接配置Jetty Http Server的最大Header长度，搜索后发现在原生的Jetty Server中，可以配置requestHeaderSize这个属性来更改默认的配置，但是估计是因为第一版的原因，Phoenix并没有提供相关的配置方法。</p>
<p>所以干脆暴力点，直接修改相关的类文件，发现在Phoenix 4.4版本中，Jetty相关的类文件是放在<code>phoenix-server-4.4.0-HBase-1.0-runnable.jar</code>这个jar包中的，然后4.4版本的Phoenix Server是依赖的9.2.7版本的Jetty Server，所以直接去github上的<a href="https://github.com/eclipse/jetty.project" target="_blank" rel="external">Jetty Project</a>下载到对应版本的<code>org.eclipse.jetty.server.HttpConfiguration.java</code>文件，修改一下默认的Header大小设置。</p>
<p>修改前：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">int</span> <span class="number">_</span>requestHeaderSize=<span class="number">8</span>*<span class="number">1024</span>; <span class="comment">//默认8KB</span></span><br></pre></td></tr></table></figure>
<p>修改后：</p>
<figure class="highlight aspectj"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">int</span> _requestHeaderSize=<span class="number">1024</span>*<span class="number">1024</span>*<span class="number">1024</span>; <span class="comment">//这次应该够用了 = =</span></span><br></pre></td></tr></table></figure>
<p>编译后直接替换掉相应的类文件，重新打包替换原来的jar包，重启Phoenix Server，搞定。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://dingguitao.github.io/2015/05/26/fix_phoenix_server_error_413/" data-id="cibj9md3x000fcrowzwl2warm" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Avatica/">Avatica</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Jetty/">Jetty</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Phoenix/">Phoenix</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-apache_phoenix_query_hbase_using_sql" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2015/05/15/apache_phoenix_query_hbase_using_sql/" class="article-date">
  <time datetime="2015-05-15T07:27:04.000Z" itemprop="datePublished">2015-05-15</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Data-Technology/">Data Technology</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/05/15/apache_phoenix_query_hbase_using_sql/">Apache Phoenix: 使用SQL查询HBase</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="I-_Apache_Phoenix_简介">I. Apache Phoenix 简介</h2><p><a href="http://phoenix.apache.org/index.html" target="_blank" rel="external">Apache Phoenix</a>（以下简称Phoenix）是架在HBase上面的一个关系型数据库层，这意味着我们可以使用传统的SQL语言来查询HBase这个NoSQL数据库中的数据，官网号称可以对百万行以上级别的表进行秒级的查询。</p>
<p>Phoenix是由Salesforce.com开源出来的项目，创立的使命在于提供一个友好的HBase数据API。现在是Apache下的开源项目，截止这篇文章诞生之日，Phoenix的版本更新到了4.3.1，已经是一个比较成熟的软件，并且社区也比较活跃，发到邮件组的问题一般都能很快得到解答，Hortonworks的产品中已经整合了Phoenix，Cloudera也在准备了。</p>
<h2 id="II-_Apache_Phoenix_初体验">II. Apache Phoenix 初体验</h2><h3 id="1-安装Phoenix">1.安装Phoenix</h3><p>Phoenix的安装步骤十分简单，<a href="http://phoenix.apache.org/installation.html" target="_blank" rel="external">官网</a>上也有更详细的介绍：</p>
<ul>
<li>1.下载最新的binary文件</li>
<li>2.将phoenix-*-server.jar复制到HBase Master和RS的CLASSPATH下</li>
<li>3.重启HBase</li>
<li>4.在phoenix的bin目录下直接执行<code>./sqlline.py zookeeper</code>即可</li>
</ul>
<h3 id="2-与HBase中的表做Mapping">2.与HBase中的表做Mapping</h3><p>如果HBase中已经建好了表，可以直接在Phoenix下创建一个View，例如在HBase下用下面这条命令创建了一张表：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator"><span class="keyword">create</span> <span class="string">'tbl'</span>, <span class="string">'cf'</span></span></span><br></pre></td></tr></table></figure>
<p>在Phoenix中，执行下面这条命令即可：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator"><span class="keyword">create</span> <span class="keyword">view</span> <span class="string">"tbl"</span> (pk <span class="built_in">VARCHAR</span> <span class="keyword">PRIMARY</span> <span class="keyword">KEY</span>, <span class="string">"cf"</span>.q1 <span class="built_in">VARCHAR</span>, <span class="string">"cf"</span>.q2 <span class="built_in">VARCHAR</span>)</span></span><br></pre></td></tr></table></figure>
<p>然后就可以在Phoenix中写SQL来查询HBase表中的数据了，可以看到Phoenix做的事情就是把数据的展现形式从HBase中的Map形式转换成了传统关系型数据库的行列形式，当然底层的数据存储格式并没有发生变化。如果HBase的表名和列族名都是大写的英文字母，那么就没有必要在Phoenix的SQL中使用双引号了。</p>
<p>如果HBase中还没有建表，可以直接在Phoenix中创建表，例如在Phoenix中执行这条命令：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator"><span class="keyword">create</span> <span class="keyword">table</span> new_tbl (pk <span class="built_in">VARCHAR</span> <span class="keyword">PRIMARY</span> <span class="keyword">KEY</span>, q1 <span class="built_in">VARCHAR</span>, q2 <span class="built_in">VARCHAR</span>)</span></span><br></pre></td></tr></table></figure>
<p>这样会在HBase中创建一个名为<em>NEW_TBL</em>的表（注意：表名是大写），有一个列族，列族名默认为<em>0</em>。</p>
<h3 id="3-批量导入数据">3.批量导入数据</h3><p>往Phoenix表中批量导入数据有三种方式：</p>
<ul>
<li>1.直接向HBase的表中导入数据</li>
<li>2.通过Phoenix提供的<code>psql.py</code>工具批量导入CSV文件</li>
<li>3.通过Phoenix提供的MapReduce程序导入CSV文件</li>
</ul>
<p>具体的步骤可以参考<a href="http://phoenix.apache.org/bulk_dataload.html" target="_blank" rel="external">官网上的教程</a>。</p>
<h3 id="4-数据查询">4.数据查询</h3><p>对Phoenix表进行数据查询也有三种方式：</p>
<ul>
<li>1.直接在Phoenix终端写SQL查询：SQL大部分的语法特性Phoenix都支持，语法也都比较正常。值得一提的是Phoenix SQL里支持group by column alias和order by column expression的写法，并且order by中出现的表达式或者列名可以不在select中出现，写的SQL可以更简洁了。一个例子：</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 传统 group by 姿势</span></span><br><span class="line"><span class="operator"><span class="keyword">select</span></span><br><span class="line">    <span class="keyword">case</span></span><br><span class="line">        <span class="keyword">when</span> col1 = <span class="string">'a'</span> <span class="keyword">then</span> <span class="string">'a'</span></span><br><span class="line">        <span class="keyword">else</span> <span class="string">'b'</span></span><br><span class="line">    <span class="keyword">end</span> <span class="keyword">as</span> col1_grp,</span><br><span class="line">    <span class="keyword">sum</span>(<span class="keyword">value</span>) <span class="keyword">as</span> value_sum</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">    <span class="keyword">table</span></span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span></span><br><span class="line">    <span class="keyword">case</span></span><br><span class="line">        <span class="keyword">when</span> col1 = <span class="string">'a'</span> <span class="keyword">then</span> <span class="string">'a'</span></span><br><span class="line">        <span class="keyword">else</span> <span class="string">'b'</span></span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"><span class="comment">-- Phoenix group by 新姿势</span></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">    <span class="keyword">case</span></span><br><span class="line">       <span class="keyword">when</span> col1 = <span class="string">'a'</span> <span class="keyword">then</span> <span class="string">'a'</span></span><br><span class="line">       <span class="keyword">else</span> <span class="string">'b'</span></span><br><span class="line">    <span class="keyword">end</span> <span class="keyword">as</span> col1_grp,</span><br><span class="line">    <span class="keyword">sum</span>(<span class="keyword">value</span>) <span class="keyword">as</span> value_sum</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">    <span class="keyword">table</span></span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span></span><br><span class="line">    col1_grp</span></span><br></pre></td></tr></table></figure>
<ul>
<li>2.使用Java API：Phoenix提供了Java API，可以通过JDBC的方式连接到Phoenix，然后执行相应的增删改查命令。一个例子：</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.sql.Connection;</span><br><span class="line"><span class="keyword">import</span> java.sql.DriverManager;</span><br><span class="line"><span class="keyword">import</span> java.sql.ResultSet;</span><br><span class="line"><span class="keyword">import</span> java.sql.SQLException;</span><br><span class="line"><span class="keyword">import</span> java.sql.PreparedStatement;</span><br><span class="line"><span class="keyword">import</span> java.sql.Statement;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">test</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> SQLException </span>&#123;</span><br><span class="line">        Statement stmt = <span class="keyword">null</span>;</span><br><span class="line">        ResultSet rset = <span class="keyword">null</span>;</span><br><span class="line">        </span><br><span class="line">        Connection con = DriverManager.getConnection(<span class="string">"jdbc:phoenix:[zookeeper]"</span>);</span><br><span class="line">        stmt = con.createStatement();</span><br><span class="line">        </span><br><span class="line">        stmt.executeUpdate(<span class="string">"create table test (mykey integer not null primary key, mycolumn varchar)"</span>);</span><br><span class="line">        stmt.executeUpdate(<span class="string">"upsert into test values (1,'Hello')"</span>);</span><br><span class="line">        stmt.executeUpdate(<span class="string">"upsert into test values (2,'World!')"</span>);</span><br><span class="line">        con.commit();</span><br><span class="line">        </span><br><span class="line">        PreparedStatement statement = con.prepareStatement(<span class="string">"select * from test"</span>);</span><br><span class="line">        rset = statement.executeQuery();</span><br><span class="line">        <span class="keyword">while</span> (rset.next()) &#123;</span><br><span class="line">            System.out.println(rset.getString(<span class="string">"mycolumn"</span>));</span><br><span class="line">        &#125;</span><br><span class="line">        statement.close();</span><br><span class="line">        con.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>3.使用<a href="http://phoenix.apache.org/server.html" target="_blank" rel="external">Query Server</a>：Phoenix从4.4版本开始会提供一个基于<a href="https://github.com/apache/incubator-calcite/blob/master/doc/avatica.md" target="_blank" rel="external">Avatica</a>的Query Server，相信这会给利用HBase或者Phoenix提供数据服务的同学们带来一个更好的选择。</li>
</ul>
<h2 id="III-_Apache_Phoenix_适用场景">III. Apache Phoenix 适用场景</h2><p>作为一个HBase之上的关系型数据库层，Phoenix有很多自己的特性，这些特性决定了它的适用场景。</p>
<h3 id="1-优势（与HBase原生API比较）">1.优势（与HBase原生API比较）</h3><ul>
<li><strong>1.SQL代替Java：</strong>SQL接口代替HBase的Java接口，就像使用了Hive就不再写MapReduce程序一样，这样可以节省很多的开发量。</li>
<li><strong>2.二级索引：</strong>对Phoenix中的表，用户也可以对Row Key之外的列建立索引，关于HBase的二级索引包括华为、360等很多的公司都做过比较多的开发工作，在Phoenix中只需要一句SQL。</li>
<li><strong>3.并行查询：</strong>HBase是为<strong><em>随机读</em></strong>的场景设计的，所以在表扫描的时候，各个region进行的是顺序扫描，Phoenix利用HBase的coprocessor实现了表的并行扫描，对<strong><em>全表扫描</em></strong>的效率提升很大。</li>
<li><strong>4.Salted Row Key：</strong>Phoenix可以对HBase Row Key的进行加盐，妈妈再也不用担心Hotspotting的问题了。</li>
</ul>
<p>此外，Phoenix还提供了像子查询、表连接这样的关系型数据库中的特性，可以发挥的空间很大。</p>
<h3 id="2-局限（与关系型数据库比较）">2.局限（与关系型数据库比较）</h3><ul>
<li>1.事务是行级别的，目前还不支持表级别的事务。</li>
<li>2.查询的优化器比较简单，不够成熟。</li>
<li>3.查询的延迟比较大，虽说已经是低延迟的服务了，但测试下来发现现在的查询效率还不适合提供OLAP的服务，OLTP没问题。</li>
</ul>
<p>总结下来，如果在你的场景中需要经常对HBase中的表进行全表扫描，并且自己又没有足够的资源通过HBase的API实现一些比较复杂的功能，那么Apache Phoenix可能是一个比较好的选择。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://dingguitao.github.io/2015/05/15/apache_phoenix_query_hbase_using_sql/" data-id="cibj9md43000scrow0zwd77uo" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/HBase/">HBase</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Phoenix/">Phoenix</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/SQL-on-HBase/">SQL on HBase</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-Kiong-Sing" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2013/01/25/Kiong-Sing/" class="article-date">
  <time datetime="2013-01-25T13:06:28.000Z" itemprop="datePublished">2013-01-25</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Kiong/">Kiong</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2013/01/25/Kiong-Sing/">唱歌跑调</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>Kiong不太会唱歌，确切地讲是不会唱歌，因为无论唱什么歌，她都能够巧妙地避开每一个正确的调，以至于后来当我们意见不统一的时候：</p>
<p>Kiong：你听不听我的？！</p>
<p>Me：不听！</p>
<p>Kiong：你确定？</p>
<p>Me：确定肯定以及一定！</p>
<p>Kiong：那我要唱歌给你听了！</p>
<p>Me（感受到了比东北话里的“你瞅啥”更强烈的生命安全威胁）：。。。且慢！仔细想想你说的好有道理 … …</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://dingguitao.github.io/2013/01/25/Kiong-Sing/" data-id="cibj9md4a0015crowe38ckxts" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-Kiong-About" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2012/12/25/Kiong-About/" class="article-date">
  <time datetime="2012-12-25T14:22:22.000Z" itemprop="datePublished">2012-12-25</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Kiong/">Kiong</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2012/12/25/Kiong-About/">关于Kiong</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>Kiong是一个人，她之前是我的学妹，现在是我女朋友，将来是我媳妇（嘿嘿嘿嘿，喊我禽兽有什么用，有本事过来打我呀）。</p>
<p>Kiong（二声）这个词来源于闽南话中“琼”的发音，Kiong的家人就是这么喊她的，后来我把新华字典翻了一遍也没有找到对应的读音和汉字，于是“Kiong”就变成了她的英文名。</p>
<p>从我认识Kiong的第一天开始，每次见到她，她总是蹦蹦跳跳有说有笑的，有她在的地方，大家也都是轻松愉快的，她的世界里好像没有烦恼二字。当我真正走进她的世界，生活中就突然多出了好多好玩的事情，于是“Kiong”就变成了一种生活态度。</p>
<p>所以我在博客中建立了“Kiong”这个分类，记录关于Kiong的一切。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://dingguitao.github.io/2012/12/25/Kiong-About/" data-id="cibj9md4c0018crow6vw6c32n" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
  
</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Data-Technology/">Data Technology</a><span class="category-list-count">5</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Kiong/">Kiong</a><span class="category-list-count">2</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/Avatica/">Avatica</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Encoding/">Encoding</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Feature-Construction/">Feature Construction</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Feature-Engineering/">Feature Engineering</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Feature-Extraction/">Feature Extraction</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/HBase/">HBase</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Hive/">Hive</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Jetty/">Jetty</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/OpenTSDB/">OpenTSDB</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Phoenix/">Phoenix</a><span class="tag-list-count">4</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/SQL-on-HBase/">SQL on HBase</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Time-Series-Database/">Time Series Database</a><span class="tag-list-count">1</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/Avatica/" style="font-size: 13.33px;">Avatica</a><a href="/tags/Encoding/" style="font-size: 10px;">Encoding</a><a href="/tags/Feature-Construction/" style="font-size: 10px;">Feature Construction</a><a href="/tags/Feature-Engineering/" style="font-size: 10px;">Feature Engineering</a><a href="/tags/Feature-Extraction/" style="font-size: 10px;">Feature Extraction</a><a href="/tags/HBase/" style="font-size: 16.67px;">HBase</a><a href="/tags/Hive/" style="font-size: 10px;">Hive</a><a href="/tags/Jetty/" style="font-size: 10px;">Jetty</a><a href="/tags/OpenTSDB/" style="font-size: 10px;">OpenTSDB</a><a href="/tags/Phoenix/" style="font-size: 20px;">Phoenix</a><a href="/tags/SQL-on-HBase/" style="font-size: 10px;">SQL on HBase</a><a href="/tags/Time-Series-Database/" style="font-size: 10px;">Time Series Database</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/06/">六月 2015</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/05/">五月 2015</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2013/01/">一月 2013</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2012/12/">十二月 2012</a><span class="archive-list-count">1</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recents</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2015/06/27/OpenTSDB-Digest-time-series-data-table-design-on-hbase/">OpenTSDB Digest: HBase 中时间序列数据表的设计</a>
          </li>
        
          <li>
            <a href="/2015/06/07/Feature-Engineering-using-Apache-Hive-HBase-and-Phoenix/">从ODS到FEATURE：Apache Hive、HBase、Phoenix三剑客之离线特征工程实战</a>
          </li>
        
          <li>
            <a href="/2015/05/28/change-phoenix-server-request-encoding-to-utf8/">Apache Phoenix Server中文乱码问题解决</a>
          </li>
        
          <li>
            <a href="/2015/05/26/fix_phoenix_server_error_413/">Apache Phoenix Server 提示 Header 过长（code 413）的解决办法</a>
          </li>
        
          <li>
            <a href="/2015/05/15/apache_phoenix_query_hbase_using_sql/">Apache Phoenix: 使用SQL查询HBase</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2015 Ding Guitao<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css" type="text/css">
  <script src="/fancybox/jquery.fancybox.pack.js" type="text/javascript"></script>


<script src="/js/script.js" type="text/javascript"></script>

  </div>
</body>
</html>